---
title: "TFM"
author: "Javier Ortiz"
date: "30 de Junio de 2017"
output: 
  word_document: null
  html_document: default
  number_sections: yes
  theme: cosmo
  highlight: default
---

******
## Carga de librerias
******
Load libraries
```{r librerias, include=FALSE}
if(! "ggplot2" %in% installed.packages()) 
  install.packages("ggplot2",dependencies = TRUE)
library(ggplot2)
if(! "data.table" %in% installed.packages()) install.packages("data.table",dependencies = TRUE)
library(data.table)
if(! "caret" %in% installed.packages()) 
  install.packages("caret",dependencies = TRUE)
library(caret)
if(! "kableExtra" %in% installed.packages()) install.packages("kableExtra",dependencies = TRUE)
library(kableExtra)
if(! "xtable" %in% installed.packages()) 
  install.packages("xtable",dependencies = TRUE)
library(xtable)
if(! "corrplot" %in% installed.packages()) 
  install.packages("corrplot",dependencies = TRUE)
library(corrplot)
if(! "lubridate" %in% installed.packages()) 
  install.packages("lubridate",dependencies = TRUE)
library(lubridate)
if(! "caret" %in% installed.packages()) 
  install.packages("caret",dependencies = TRUE)
library(caret)
if(! "dplyr" %in% installed.packages()) 
  install.packages("dplyr",dependencies = TRUE)
library(dplyr)
if(! "ROCR" %in% installed.packages()) 
  install.packages("ROCR",dependencies = TRUE)
library(ROCR)
if(! "xgboost" %in% installed.packages()) 
  install.packages("xgboost",dependencies = TRUE)
library(xgboost)
if(! "Matrix" %in% installed.packages()) 
  install.packages("Matrix",dependencies = TRUE)
library(Matrix)
if(! "doParallel" %in% installed.packages()) 
  install.packages("doParallel",dependencies = TRUE)
library(doParallel)		# parallel processing
rm(list = ls());
gc();
```

******
## Carga de datos y análisis descriptivo de los datos
******
```{r descriptiva, echo=FALSE}
# Cargamos los datos datasets macro, train y test
directorio_trabajo <- setwd("C:/Users/Javier Ortiz/Desktop/Master Big Data/TFM/TFM-UEMC")
getwd()

# Con la libreria DoParallel vamos a emplear el proceso en paralelo para los 4
# procesadores de mi máquina (Windows 10)
registerDoParallel(4)
getDoParWorkers()

macro <- fread(file="Estudio preliminar/Datos/macro.csv", header=TRUE, sep=",", na.strings="NA", stringsAsFactors=TRUE)

train <- fread(file="Estudio preliminar/Datos/train.csv", header=TRUE, sep=",", na.strings="NA", stringsAsFactors=TRUE)

test <- fread(file="Estudio preliminar/Datos/test.csv", header=TRUE, sep=",", na.strings="NA", stringsAsFactors=TRUE)

#Resumen del dataset train
dim(train)
head(train)
tail(train)
summary(train)
str(train)

# Precios por año
boxplot(train$price_doc~year(train$timestamp),data=train, main="Precios x año", xlab="Año", ylab="Precio")

# Precios por sub-area
plot(train$price_doc~train$sub_area, main="Precios x sub area", xlab="Sub-area", ylab="Precio", col="red")

train %>% 
    group_by(sub_area) %>%
    summarize(n=n()) %>%
    ggplot(aes(x=reorder(sub_area, n), y=n)) + 
    geom_bar(stat='identity') + 
    coord_flip() +
    labs(y='Numero de transacciones', x='', title='Número de transacciones por Area')

# Precios por metros cuadrado
ggplot(aes(x=full_sq, y=price_doc), data = train) + 
    geom_point(color='red', alpha=0.5) +
    labs(x='Area', y='Precio', title='Precio por area en metros cuadrados')

# Precios por año construcción
ggplot(aes(x=build_year, y=price_doc), data=train) + 
    geom_point(color='red')

train$fecha <- as_datetime(train$timestamp)

train %>% 
    filter(year(build_year) > 1691 & year(build_year) < 2018) %>% 
    ggplot(aes(x=year(build_year))) + geom_histogram(fill='red',bins=10) + 
    ggtitle('Distribución por año de construcción')

ggplot(aes(x=sub_area, y=price_doc), data=train) + 
    geom_point(color='red')

# Habitaciones por propiedad
ggplot(train, aes(x = num_room)) + geom_density(adjust = 10)
ggplot(train, aes(x = num_room)) + geom_histogram(binwidth = 2)
ggplot(train, aes(x = num_room)) + geom_histogram(fill='red', bins=20) + 
    ggtitle('Distribución de habitaciones')

ggplot(train, aes(x = price_doc, y = sub_area)) + geom_boxplot()
ggplot(train, aes(x = product_type, y = life_sq)) + geom_boxplot() + coord_flip()
ggplot(train, aes(x = product_type)) + geom_dotplot(dotsize = 0.4)

train %>%
    group_by(fecha = year(timestamp)) %>%
    summarize(precio_medio = median(price_doc)) %>%
    ggplot(aes(x = fecha, y = precio_medio)) +
    geom_line(color = 'red') +
    geom_smooth(method = 'lm', color = 'grey', alpha = 0.7) + 
    ggtitle('Precio medio en el tiempo')
```

******
## Limpieza del dataset y selección de datos
******

```{r clean, echo=FALSE}
# Limpiamos las variables del dataset train que vamos a emplear
train$full_sq[is.na(train$full_sq)] <- 0
train$life_sq[is.na(train$life_sq)] <- 0
train$build_year[is.na(train$build_year)] <- 0
train$floor[is.na(train$floor)] <- 0
train$max_floor[is.na(train$max_floor)] <- 0
train$material[is.na(train$material)] <- 0
train$num_room[is.na(train$num_room)] <- 0
train$kitch_sq[is.na(train$kitch_sq)] <- 0
train$state[is.na(train$state)] <- 0
train$raion_popul[is.na(train$raion_popul)] <- 0
train$work_all[is.na(train$work_all)] <- 0

train[,timestamp := as.Date(timestamp)]
train <- sapply(train,as.numeric)

# Limpiamos las variables del dataset test que vamos a emplear
test$full_sq[is.na(test$full_sq)] <- 0
test$life_sq[is.na(test$life_sq)] <- 0
test$build_year[is.na(test$build_year)] <- 0
test$floor[is.na(test$floor)] <- 0
test$max_floor[is.na(test$max_floor)] <- 0
test$material[is.na(test$material)] <- 0
test$num_room[is.na(test$num_room)] <- 0
test$kitch_sq[is.na(test$kitch_sq)] <- 0
test$state[is.na(test$state)] <- 0
test$raion_popul[is.na(test$raion_popul)] <- 0
test$work_all[is.na(test$work_all)] <- 0

# Creamos la variable price_doc en el dataset de test
test$price_doc <- 0

test[,timestamp := as.Date(timestamp)]
test <- sapply(test,as.numeric)

# Limpiamos los nulos de los campos que vamos a utilizar del dataset macro
macro$deposits_value[is.na(macro$deposits_value)] <- 0
macro$deposits_growth[is.na(macro$deposits_growth)] <- 0
macro$deposits_rate[is.na(macro$deposits_rate)] <- 0
macro$mortgage_value[is.na(macro$mortgage_value)] <- 0
macro$mortgage_growth[is.na(macro$mortgage_growth)] <- 0
macro$mortgage_rate[is.na(macro$mortgage_rate)] <- 0
macro$income_per_cap[is.na(macro$income_per_cap)] <- 0
macro$real_dispos_income_per_cap_growth[is.na(macro$real_dispos_income_per_cap_growth)] <- 0
macro$salary[is.na(macro$salary)] <- 0
macro$salary_growth[is.na(macro$salary_growth)] <- 0
macro$fixed_basket[is.na(macro$fixed_basket)] <- 0
macro$unemployment[is.na(macro$unemployment)] <- 0
macro$employment[is.na(macro$employment)] <- 0
macro$invest_fixed_capital_per_cap[is.na(macro$invest_fixed_capital_per_cap)] <- 0
macro$invest_fixed_assets[is.na(macro$invest_fixed_assets)] <- 0

macro[,timestamp := as.Date(timestamp)]
macro <- sapply(macro,as.numeric)

#Selección de datos para el subconjunto de datos de train
train_sub <- subset(train,select = c(id,timestamp,full_sq,life_sq,floor,max_floor,material,build_year,num_room,kitch_sq,state,raion_popul,work_all,price_doc))

# Creo el subconjunto de datos de macro con las variables que me parecen más relevantes
macro_sub <- subset(macro,select = c(timestamp,deposits_value,deposits_growth,deposits_rate,mortgage_value,mortgage_growth,mortgage_rate,income_per_cap,real_dispos_income_per_cap_growth,salary,salary_growth,fixed_basket,unemployment,employment,invest_fixed_capital_per_cap,invest_fixed_assets))

#Selección de datos para el subconjunto de datos de test
test_sub <- subset(test,select = c(id,timestamp,full_sq,life_sq,floor,max_floor,material,build_year,num_room,kitch_sq,state,raion_popul,work_all,price_doc))

```

******
## Análisis exploratorio apoyado en algún método NO supervisado (Clustering)
******
Normalización de atributos
```{r exploratory,eval=TRUE, echo=FALSE}

# Unión de los subconjuntos de datos train y macro
train_final <- merge(train_sub,macro_sub,"timestamp")

#Gráficos mod3 u1 pág 115
plot(train_final$price_doc,train_final$full_sq ,main = "Full-sq Precio", pch=20)
ggplot(aes(x=price_doc), data=train_final) + 
    geom_density(fill='red', color='red') + 
    facet_grid(~full_sq) + 
    scale_x_continuous(trans='log')

# Matriz de correlación
matCor <- cor(subset(train,select = c(id,full_sq,life_sq,floor,max_floor,material,build_year,num_room,kitch_sq,state,raion_popul,work_all,price_doc)))

corrplot(cor(matCor, use="complete.obs"),method = "number",order ="alphabet")
corrplot(cor(macro_sub, use="complete.obs"),method = "number",order ="alphabet")

# Unión de los subconjuntos de datos test y macro
test_final <- merge(test_sub,macro_sub,"timestamp")

#Realizamos 15 iteraciones empleando el indicador withniss obtendremos la suma de los cuadrados de las distancias entre los centros determinados por el algoritmo kmeans y los puntos que están dentro de cada cluster.
kmeans(train_sub,centers=9)$tot.withinss
wss <- (nrow(train_sub)-1)*sum(apply(train_sub,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(train_sub,
                                       centers=i)$withinss)

#Dibujamos el gráfico para ver cual es el número de clústers más correctos para elegir
plot(1:15, wss, type="b", xlab="Numero de Clusters",
     ylab="Sumas de cuadrados dentro de los grupos",
     main="Num de clusters óptimos",
     pch=20, cex=2)

kmeans.result <- kmeans(train_sub, centers=5)
centros <- kmeans.result$centers[kmeans.result$cluster,]
distancias <- sqrt(rowSums((train_sub - centros)^2))
outliers <- order(distancias, decreasing=T)[1:5]
train_sub[outliers,]

#Detección de outliers
plot(train_sub[,c("full_sq","life_sq","floor","max_floor","material","build_year","num_room","state","raion_popul","work_all","kitch_sq","price_doc")], main="Detección de outliers", pch="o", col=kmeans.result$cluster, cex=0.3)
points(kmeans.result$centers[,c("full_sq","life_sq","floor","max_floor","material","build_year","num_room","state","raion_popul","work_all","kitch_sq","price_doc")], col=1:5, pch=8, cex=1.5)
points(train_sub[outliers,c("full_sq","life_sq","floor","max_floor","material","build_year","num_room","state","raion_popul","work_all","kitch_sq","price_doc")], col=4, pch="+", cex=1.5)
points(matrix(colMeans(train_sub),nrow=1,ncol=2),cex=3,col=12,pch=19)

```

******
## Parámetros comunes para todas las ejecuciones
******
```{r PARAMS, echo=FALSE}
# Parametrización training control
set.seed(1000);
control <- trainControl(method = "repeatedcv",   
                                 number = 5,
                                 allowParallel = TRUE)
```


******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (glmboost - Caret)
******
```{r GLMBOOST, echo=FALSE}
glmboost.model <- train(price_doc ~ .,
                            data = train_sub,
                            method = "glmboost",
                            trControl = control)

# Predicción de la variable price_doc
glmboost.pred <- predict(glmboost.model,test_final)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=glmboost.pred), "Estudio final/Datos/submission_glmboost.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)

```

******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (gbm)
******
Realizamos las misma operaciones que para el primer modelo
```{r GBM, echo=FALSE}
gbm.model <- train(price_doc ~ .,
                            data = train_final,
                            method = "gbm",
                            trControl = control)

# Predicción de la variable price_doc
gbm.pred <- predict(gbm.model,test_final)

gbm.model$bestTune
plot(gbm.model)  		# Plot the performance of the training models
res <- gbm.model$results
res

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=gbm.pred), "Estudio final/Datos/submission_gbm.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
```

******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (lm)
******
Realizamos las misma operaciones que para el primer modelo
```{r LM, echo=FALSE}
lm.model <- train(price_doc ~ .,
                            data = train_final,
                            method = "lm",
                            #tuneLength = 10,
                            trControl = control)

# Predicción de la variable price_doc
lm.pred <- predict(lm.model,test_final)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=lm.pred), "Estudio final/Datos/submission_lm.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
```
******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (xgbTree)
******
```{r XGBTREE, echo=FALSE}
xgbtree.model <- train(price_doc ~ .,
                            data = train_sub,
                            method = "xgbTree",
                            trControl = control)

# Predicción de la variable price_doc
xgbtree.pred <- predict(xgbtree.model,test_final)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=glmboost.pred), "Estudio final/Datos/submission_xgbtree.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)

```
******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (xgblinear)
******
```{r XGBLINEAR, echo=FALSE}
xgblinear.model <- train(price_doc ~ .,
                            data = train_sub,
                            method = "xgbLinear",
                            trControl = control)

# Predicción de la variable price_doc
xgblinear.pred <- predict(xgblinear.model,test_final)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=glmboost.pred), "Estudio final/Datos/submission_xgblinear.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)

```
******
## Evaluación y comparación de dichos modelos realizados
******
Gracias al paquete Caret podremos comparar los modelos.
```{r comparation, echo=FALSE}
models <- list(glmboost.model,gbm.model,lm.model,xgbtree.model,xgblinear.model)
compar.models <- resamples(models)
summary(compar.models)
```
******
## Gráficos
******
Mediante el gráfico podemos ver como el modelo 2 (RMSE - Rsquared) tiene una precisión mayor que el primer modelo.
```{r graphics, echo=FALSE}
dotplot(compar.models)
```