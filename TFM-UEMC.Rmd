---
title: "TFM Sberbank Russian Housing Market"
author: "Francisco Javier Ortiz Escuchas"
date: "30 de Junio de 2017"
output: 
  word_document: null
  html_document: default
  number_sections: yes
  theme: cosmo
  highlight: default
---

******
## Carga de librerias
******
Load libraries (ggplot2,data.table,caret,corrplot,lubridate,dplyr,doParallel,sp,rgdal,magrittr,plyr)
```{r librerias, include=FALSE}
if(! "ggplot2" %in% installed.packages()) 
  install.packages("ggplot2",dependencies = TRUE)
library(ggplot2)
if(! "data.table" %in% installed.packages()) install.packages("data.table",dependencies = TRUE)
library(data.table)
if(! "caret" %in% installed.packages()) 
  install.packages("caret",dependencies = TRUE)
library(caret)
if(! "corrplot" %in% installed.packages()) 
  install.packages("corrplot",dependencies = TRUE)
library(corrplot)
if(! "lubridate" %in% installed.packages()) 
  install.packages("lubridate",dependencies = TRUE)
library(lubridate)
if(! "dplyr" %in% installed.packages()) 
  install.packages("dplyr",dependencies = TRUE)
library(dplyr)
if(! "doParallel" %in% installed.packages()) 
  install.packages("doParallel",dependencies = TRUE)
library(doParallel)		# parallel processing

#Limpiamos el espacio de trabajo
rm(list = ls());
gc();
```
******
## Carga de datos y análisis descriptivo de los datos
******
```{r descriptiva, echo=FALSE, message= FALSE}
# Cargamos los datos datasets macro, train y test
mainDir <- "c:/Users/Javier Ortiz/Desktop/Master Big Data/TFM/TFM-UEMC/Estudio_preliminar/Datos"

if (dir.exists(mainDir)){
    setwd(file.path(mainDir))
} else {
    dir.create(file.path(mainDir))
    setwd(file.path(mainDir))
}

directorio_trabajo <- setwd(mainDir)
getwd()

# Con la libreria DoParallel vamos a emplear el proceso en paralelo para los 
# procesadores de mi máquina (Windows 10) que vamos a detectar automáticamente
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Cargamos los ficheros (macro,train y test) con la función fread
macro <- fread(file=paste(directorio_trabajo,"/macro.csv",sep = "", collapse = NULL), header=TRUE, sep=",", na.strings="NA", stringsAsFactors=TRUE)

train <- fread(file=paste(directorio_trabajo,"/train.csv",sep = "", collapse = NULL), header=TRUE, sep=",", na.strings="NA", stringsAsFactors=TRUE)

test <- fread(file=paste(directorio_trabajo,"/test.csv",sep = "", collapse = NULL), header=TRUE, sep=",", na.strings="NA", stringsAsFactors=TRUE)

coord <- fread(file=paste(directorio_trabajo,"/coord.csv",sep = "", collapse = NULL), header=TRUE, sep=",", na.strings="NA", stringsAsFactors=TRUE)

#Resumen del dataset train
dim(train)
#summary(train) Comentado para evitar que el informe sea muy extenso.
str(train)

# Precios por año
boxplot(train$price_doc~year(train$timestamp),data=train, main="Precios x año", xlab="Año", ylab="Precio")

# Precios por sub-area
plot(train$price_doc~train$sub_area, main="Precios x sub area", xlab="Sub-area", ylab="Precio", col="red")

# Número de transacciones por area
train %>% 
    group_by(sub_area) %>%
    summarize(n=n()) %>%
    ggplot(aes(x=reorder(sub_area, n), y=n)) + 
    geom_bar(stat='identity') + 
    coord_flip() +
    labs(y='Numero de transacciones', x='Sub-Area', title='Número de transacciones por Area')

# Precios por metros cuadrado
ggplot(aes(x=full_sq, y=price_doc), data = train) + 
    geom_point(color='red', alpha=0.5) +
    xlim(0, 200) + 
    labs(x='M2', y='Precio', title='Precio por area en metros cuadrados')

# Precios por año construcción
ggplot(aes(x=build_year, y=price_doc), data=train) + 
    xlim(1870, 2017) + 
    geom_point(color='red')

#train$fecha <- as_datetime(train$timestamp)

# Habitaciones por propiedad
ggplot(train, aes(x = num_room)) + geom_histogram(fill='red', bins=20) + 
    xlim(0,12) + 
    ggtitle('Distribución de habitaciones')

# Precios x area
ggplot(train, aes(x = price_doc, y = sub_area)) + geom_boxplot()

# Tipo producto x metro cuadrado
ggplot(train, aes(x = product_type, y = life_sq)) + geom_boxplot() + coord_flip()

# Precio medio por tiempo
train %>%
    group_by(fecha = year(timestamp)) %>%
    summarize(precio_medio = median(price_doc)) %>%
    ggplot(aes(x = fecha, y = precio_medio)) +
    geom_line(color = 'red') +
    geom_smooth(method = 'lm', color = 'grey', alpha = 0.7) + 
    ggtitle('Precio medio en el tiempo')

# Depositos medio por tiempo
macro %>%
    group_by(fecha = year(timestamp)) %>%
    summarize(depositos = median(deposits_value)) %>%
    ggplot(aes(x = fecha, y = depositos)) +
    geom_line(color = 'red') +
    geom_smooth(method = 'lm', color = 'grey', alpha = 0.7) + 
    ggtitle('Depositos medios en el tiempo')
```
******
## Limpieza del dataset y selección de datos
******

```{r clean, echo=FALSE, message= FALSE}
# Limpiamos las variables del dataset train que vamos a emplear
train$full_sq[is.na(train$full_sq)] <- 0
train$life_sq[is.na(train$life_sq)] <- 0
train$build_year[is.na(train$build_year)] <- 0
train$floor[is.na(train$floor)] <- 0
train$max_floor[is.na(train$max_floor)] <- 0
train$material[is.na(train$material)] <- 0
train$num_room[is.na(train$num_room)] <- 0
train$kitch_sq[is.na(train$kitch_sq)] <- 0
train$state[is.na(train$state)] <- 0
train$raion_popul[is.na(train$raion_popul)] <- 0
train$work_all[is.na(train$work_all)] <- 0

train[,timestamp := as.Date(timestamp)]
train <- sapply(train,as.numeric)

# Limpiamos las variables del dataset test que vamos a emplear
test$full_sq[is.na(test$full_sq)] <- 0
test$life_sq[is.na(test$life_sq)] <- 0
test$build_year[is.na(test$build_year)] <- 0
test$floor[is.na(test$floor)] <- 0
test$max_floor[is.na(test$max_floor)] <- 0
test$material[is.na(test$material)] <- 0
test$num_room[is.na(test$num_room)] <- 0
test$kitch_sq[is.na(test$kitch_sq)] <- 0
test$state[is.na(test$state)] <- 0
test$raion_popul[is.na(test$raion_popul)] <- 0
test$work_all[is.na(test$work_all)] <- 0

# Creamos la variable price_doc en el dataset de test
test$price_doc <- 0

test[,timestamp := as.Date(timestamp)]
test <- sapply(test,as.numeric)

# Limpiamos los nulos de los campos que vamos a utilizar del dataset macro
macro$deposits_value[is.na(macro$deposits_value)] <- 0
macro$deposits_growth[is.na(macro$deposits_growth)] <- 0
macro$deposits_rate[is.na(macro$deposits_rate)] <- 0
macro$mortgage_value[is.na(macro$mortgage_value)] <- 0
macro$mortgage_growth[is.na(macro$mortgage_growth)] <- 0
macro$mortgage_rate[is.na(macro$mortgage_rate)] <- 0
macro$income_per_cap[is.na(macro$income_per_cap)] <- 0
macro$real_dispos_income_per_cap_growth[is.na(macro$real_dispos_income_per_cap_growth)] <- 0
macro$salary[is.na(macro$salary)] <- 0
macro$salary_growth[is.na(macro$salary_growth)] <- 0
macro$fixed_basket[is.na(macro$fixed_basket)] <- 0
macro$unemployment[is.na(macro$unemployment)] <- 0
macro$employment[is.na(macro$employment)] <- 0
macro$invest_fixed_capital_per_cap[is.na(macro$invest_fixed_capital_per_cap)] <- 0
macro$invest_fixed_assets[is.na(macro$invest_fixed_assets)] <- 0

macro[,timestamp := as.Date(timestamp)]
macro <- sapply(macro,as.numeric)

#Selección de datos para el subconjunto de datos de train
train_sub <- subset(train,select = c(id,timestamp,full_sq,life_sq,floor,max_floor,material,build_year,num_room,kitch_sq,state,raion_popul,work_all,price_doc))

# Creo el subconjunto de datos de macro con las variables que me parecen más relevantes
macro_sub <- subset(macro,select = c(timestamp,deposits_value,deposits_growth,deposits_rate,mortgage_value,mortgage_growth,mortgage_rate,income_per_cap,real_dispos_income_per_cap_growth,salary,salary_growth,fixed_basket,unemployment,employment,invest_fixed_capital_per_cap,invest_fixed_assets))

#Selección de datos para el subconjunto de datos de test
test_sub <- subset(test,select = c(id,timestamp,full_sq,life_sq,floor,max_floor,material,build_year,num_room,kitch_sq,state,raion_popul,work_all,price_doc))
```
******
## Análisis exploratorio apoyado en algún método NO supervisado (Clustering)
******
Normalización de atributos
```{r exploratory,eval=TRUE, echo=FALSE}
# Unión de los subconjuntos de datos train y macro
train_final <- merge(train_sub,macro_sub,"timestamp")

# Matriz de correlación
matCor <- cor(subset(train,select = c(id,full_sq,life_sq,floor,max_floor,material,build_year,num_room,kitch_sq,state,raion_popul,work_all,price_doc)))

#Matriz de valores de train
corrplot(cor(matCor, use="complete.obs"),method = "number",order ="alphabet")

# Matriz de valores de macro
corrplot(cor(macro_sub, use="complete.obs"),method = "number",order ="alphabet")

# Unión de los subconjuntos de datos test y macro
test_final <- merge(test_sub,macro_sub,"timestamp")

#Realizamos 15 iteraciones empleando el indicador withniss obtendremos la suma de los cuadrados de las distancias entre los centros determinados por el algoritmo kmeans y los puntos que están dentro de cada cluster.
kmeans(train_sub,centers=9)$tot.withinss
wss <- (nrow(train_sub)-1)*sum(apply(train_sub,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(train_sub,
                                       centers=i)$withinss)

#Dibujamos el gráfico para ver cual es el número de clústers más correcto para elegir
plot(1:15, wss, type="b", xlab="Numero de Clusters",
     ylab="Sumas de cuadrados dentro de los grupos",
     main="Num de clusters óptimos",
     pch=20, cex=2)

kmeans.result <- kmeans(train_sub, centers=5)
centros <- kmeans.result$centers[kmeans.result$cluster,]
distancias <- sqrt(rowSums((train_sub - centros)^2))
outliers <- order(distancias, decreasing=T)[1:5]
train_sub[outliers,]

#Detección de outliers
plot(train_sub[,c("full_sq","life_sq","floor","max_floor","material","build_year","num_room","state","raion_popul","work_all","kitch_sq","price_doc")], main="Detección de outliers", pch="o", col=kmeans.result$cluster, cex=0.3, xlim=c(0, 750))
points(kmeans.result$centers[,c("full_sq","life_sq","floor","max_floor","material","build_year","num_room","state","raion_popul","work_all","kitch_sq","price_doc")], col=1:5, pch=8, cex=1.5)
points(train_sub[outliers,c("full_sq","life_sq","floor","max_floor","material","build_year","num_room","state","raion_popul","work_all","kitch_sq","price_doc")], col=4, pch="+", cex=1.5)
points(matrix(colMeans(train_sub),nrow=1,ncol=2),cex=3,col=12,pch=19)
```
******
## Parámetros comunes para todas las ejecuciones
******
```{r PARAMS, echo=FALSE, message= FALSE}
# Parametrización training control
control <- trainControl(method = "repeatedcv",   
                                 number = 6)

```
******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (glmboost - Caret)
******
```{r GLMBOOST, echo=FALSE, message= FALSE}
glmboost.model <- train(price_doc ~ .,
                            data = train_final,
                            method = "glmboost",
                            trControl = control,
                            metric='RMSE',
                            maximize=FALSE)

# Predicción de la variable price_doc
glmboost.pred <- predict(glmboost.model,test_final)

# Gráfico de rendimiento del modelo gmlboost
glmboost.model$bestTune
plot(glmboost.model)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=glmboost.pred), "Estudio_final/Datos/submission_glmboost.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)

```
******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (gbm - Caret)
******
Realizamos las misma operaciones que para el primer modelo
```{r GBM, echo=FALSE, message= FALSE}
#Grid <- expand.grid(n.trees = seq(50,1000,50), interaction.depth = c(30), shrinkage = c(0.1), n.minobsinnode = 3)
gbm.model <- train(price_doc ~ .,
                            data = train_final,
                            method = "gbm",
                            trControl = control,
                            #tuneGrid = Grid,
                            metric='RMSE',
                            maximize=FALSE)

# Predicción de la variable price_doc
gbm.pred <- predict(gbm.model,test_final)

# Gráfico de rendimiento del modelo gbm
gbm.model$bestTune
plot(gbm.model)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=gbm.pred), "Estudio_final/Datos/submission_gbm.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
```
******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (lm)
******
Realizamos las misma operaciones que para el primer modelo
```{r LM, echo=FALSE, message= FALSE}
lm.model <- train(price_doc ~ .,
                            data = train_final,
                            method = "lm",
                            trControl = control,
                            intercept = TRUE)

# Predicción de la variable price_doc
lm.pred <- predict(lm.model,test_final)

# Gráfico de rendimiento del modelo lm
lm.model$bestTune
summary(lm.model)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=lm.pred), "Estudio_final/Datos/submission_lm.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
```
******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (xgbTree)
******
```{r XGBTREE, echo=FALSE, message= FALSE}
xgbtree.model <- train(price_doc ~ .,
                            data = train_final,
                            method = "xgbTree",
                            trControl = control)

# Predicción de la variable price_doc
xgbtree.pred <- predict(xgbtree.model,test_final)

# Gráfico de rendimiento del modelo xgbtree
xgbtree.model$bestTune
plot(xgbtree.model)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=xgbtree.pred), "Estudio_final/Datos/submission_xgbtree.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
```
******
## Selección de variables, elección, construcción y optimización de al menos dos modelos machine Learning supervisados distintos (xgblinear)
******
```{r XGBLINEAR, echo=FALSE, message= FALSE}
xgblinear.model <- train(price_doc ~ .,
                            data = train_final,
                            method = "xgbLinear",
                            trControl = control)

# Predicción de la variable price_doc
xgblinear.pred <- predict(xgblinear.model,test_final)

# Gráfico de rendimiento del modelo xgblinear
xgblinear.model$bestTune
plot(xgblinear.model)

# Devolvemos los valores id transancción y precio en un fichero tipo csv
write.table(data.table(id=test_final$id, price_doc=xgblinear.pred), "Estudio_final/Datos/submission_xgblinear.csv", sep=",", dec=".", quote=FALSE, row.names=FALSE)
```
******
## Evaluación y comparación de dichos modelos realizados
******
Gracias al paquete Caret podremos comparar los modelos.
```{r comparation, echo=FALSE, message= FALSE}
models <- list(glmboost.model,gbm.model,lm.model,xgbtree.model,xgblinear.model)
compar.models <- resamples(models)
summary(compar.models)
```
******
## Gráficos comparativo de eficienta de los modelos
******
Mediante el gráfico podemos ver como el modelo 3 (RMSE - Rsquared) tiene una precisión mayor que los otros modelos.
```{r graphics, echo=FALSE, message= FALSE}
dotplot(compar.models)

#Paramos la paralelización
stopCluster(cl)
```